# -*- coding: utf-8 -*-
"""MNIST_classifier_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zD74bR0JO2GceFci_JUv0sPO2V8_NUpL

# MNIST Classifier

### Importing the libraries
"""
# import pickle
# from sklearn.preprocessing import StandardScaler
# from sklearn.model_selection import train_test_split
import numpy as np
# import pandas as pd
# import os
# import matplotlib.pyplot as plt
# from weights import *

"""## Part 1 - Data Preprocessing

### Importing the dataset
"""

# dataset = pd.read_csv('./train.csv')

# X = dataset.iloc[1:, 1:].values
# y = dataset.iloc[1:, 0].values


"""### Encoding categorical data

Label Encoding the dependent variable column
"""


# def one_hot(y):
#     '''
#     Create 2-d numpy array of y.size
#     number of rows and y.max() + 1
#     number of columns. y.max() returns
#     the highest element in the array (9),
#     accounting for a space capable of fitting
#     bits of data (1s and 0s) for the second
#     to the largest element (8) with index value
#     8. To account for the highest element which
#     would occupy the entire fixed array size, we
#     add 1 to make for index 9 of an array of values
#     spanning from index 0 to index 9 -> 10 values
#     '''

#     one_hot_y = np.zeros((y.size, y.max() + 1))

#     '''
#   Accessing all the elements which are arrays at once
#   and targeting index values corresponding to the y values,
#   converting them to 1 for all the values at once; hence the
#   reason for using y which is an array as the second argument
#   for the bracket notation
#   '''
#     one_hot_y[np.arange(y.size), y] = 1
#     one_hot_y = one_hot_y.T
#     return one_hot_y


"""### Splitting the dataset into the Training set and Test set"""

# Split data to train and test portions in the ratio 0.85 : 0.15
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.20, random_state=1)

# print(y_train)

# data = np.array(X_train)
# m, n = data.shape
# print(m)

# """### Feature Scaling"""

# sc = StandardScaler()

# Storing unscaled values of X for future purposes
# X_train_unscaled = X_train
# X_test_unscaled = X_test

# Apply feature scaling to all the rows and columns.
# X_train = X_train/255
# X_test = X_test/255


"""## Part 2 - Building the ANN from Scratch

### Initializing the Weights and Biases
Weights and biases would be initialized to vector matrices of values ranging from -0.5 to +0.5, with dimesions relative to the number of neurons in the different layers of the network.
"""

# Transposing matrices after splitting and feature scaling to avoid conflict
# during model's computation process

# X_train = X_train.T
# X_test = X_test.T


# def init_params():
#     W1 = np.random.rand(10, 784) - 0.5
#     b1 = np.random.rand(10, 1) - 0.5
#     W2 = np.random.rand(10, 10) - 0.5
#     b2 = np.random.rand(10, 1) - 0.5
#     W3 = np.random.rand(10, 10) - 0.5
#     b3 = np.random.rand(10, 1) - 0.5
#     return W1, b1, W2, b2, W3, b3


"""### Defining Activation Functions

"""


# def ReLu(Z):
#     return np.maximum(Z, 0)


# def softmax(Z):
#     return np.exp(Z) / sum(np.exp(Z))


# def ReLU_deriv(Z):
#     return Z > 0


"""### Defining Forward Propagation Procedure"""


# def forward_prop(W1, b1, W2, b2, W3, b3, X):
#     Z1 = W1.dot(X) + b1
#     A1 = ReLu(Z1)
#     Z2 = W2.dot(A1) + b2
#     A2 = ReLu(Z2)
#     Z3 = W3.dot(A2) + b3
#     A3 = softmax(Z3)
#     return Z1, A1, Z2, A2, Z3, A3


"""### Defining Backward Propagation Procedure"""


# def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):
#     dZ3 = A3 - one_hot(y)
#     dW3 = (1/m) * dZ3.dot(A2.T)
#     db3 = (1/m) * np.sum(dZ3)
#     dZ2 = ReLU_deriv(Z2) * ((W3.T).dot(dZ3))
#     dW2 = (1/m) * dZ2.dot(A1.T)
#     db2 = (1/m) * np.sum(dZ2)
#     dZ1 = ReLU_deriv(Z1) * ((W2.T).dot(dZ2))
#     dW1 = (1/m) * dZ1.dot(X.T)
#     db1 = (1/m) * np.sum(dZ1)
#     return dW1, db1, dW2, db2, dW3, db3


# def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):
#     W1 = W1 - alpha * dW1
#     b1 = b1 - alpha * db1
#     W2 = W2 - alpha * dW2
#     b2 = b2 - alpha * db2
#     W3 = W3 - alpha * dW3
#     b3 = b3 - alpha * db3
#     return W1, b1, W2, b2, W3, b3


# def get_predictions(A3):
#     # Returns the indices of the maximum element along each
#     return np.argmax(A3, 0)
# column, hence the reason for the 'axis' element taking
# value 0. This method was implemented because the A3 matrix
# has a dimension (10*m) -> 10 rows signifying the 10 output
# neurons and m representing the number of outputs for the m
# number of inputs passed.
# If axis of value one was used, we'd be comparing values along
# different rows, returning in an array, maximum values of each
# row


# def get_accuracy(predictions, y):
#     print(predictions, y)
#     # Compares the value in the actual result with
#     # the prediction, giving a 1 for true (equality)
#     # and 0 for false. The more ones, the more the
#     # sum of the array of 1s and 0s generated, the
#     # higher the fraction
#     return np.sum(predictions == y) / y.size


# def gradient_descent(X, y, alpha, epoch):
#     W1, b1, W2, b2, W3, b3 = init_params()
#     for i in range(epoch):
#         Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)
#         dW1, db1, dW2, db2, dW3, db3 = backward_prop(
#             Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y)
#         W1, b1, W2, b2, W3, b3 = update_params(
#             W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)

#         # Viewing prediction accuracy progression during training
#         if i % 20 == 0:
#             print(f"Epoch: {i}")
#             predictions = get_predictions(A3)
#             print(f"Accuracy score: {get_accuracy(predictions, y)}")
#     return W1, b1, W2, b2, W3, b3


"""## Part 3 - Training the ANN"""

# W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, y_train, 0.1, 1000)


class Classify:

    def __init__(self, W1, b1, W2, b2, W3, b3):
        self.W1 = W1
        self.W2 = W2
        self.W3 = W3
        self.b1 = b1
        self.b2 = b2
        self.b3 = b3

    def __str__(self):
        return f"{self.W1}, {self.W2}, {self.W3}, {self.b1}, {self.b2}, {self.b3}"

    def ReLu(self, Z):
        return np.maximum(Z, 0)

    def softmax(self, Z):
        return np.exp(Z) / sum(np.exp(Z))

    def get_predictions(self, A3):
        # Returns the indices of the maximum element along each
        return np.argmax(A3, 0)

    def forward_prop(self, X):
        Z1 = self.W1.dot(X) + self.b1
        A1 = self.ReLu(Z1)
        Z2 = self.W2.dot(A1) + self.b2
        A2 = self.ReLu(Z2)
        Z3 = self.W3.dot(A2) + self.b3
        A3 = self.softmax(Z3)
        return Z1, A1, Z2, A2, Z3, A3

    def make_predictions(self, X):
        _, _, _, _, _, A3 = self.forward_prop(X)
        predictions = self.get_predictions(A3)
        return predictions


# pred = Classify(W1, b1, W2, b2, W3, b3)

# # # print(params)

# with open('model.pckl', 'wb') as f:
#     pickle.dump(pred, f)


"""## Part 4 - Making the predictions and evaluating the model

### Predicting the result of a single observation
"""

# print(pred)


# def make_predictions(X, W1=params.W1, b1=params.b1, W2=params.W2, b2=params.b3, W3=params.W3, b3=params.b3):
#     _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)
#     predictions = get_predictions(A3)
#     return predictions


# Test
# print(pred.make_predictions(X_test[:, 0, None]))

# current_image = X_test[:, 0, None]
# current_image = current_image.reshape((28, 28))
# plt.close('all')
# plt.imshow(current_image, cmap='Greys')
# plt.show()
